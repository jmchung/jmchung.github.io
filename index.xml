<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>https://jmchung.github.io/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Tue, 07 May 2019 00:51:00 +0800</lastBuildDate>
    
	<atom:link href="https://jmchung.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Perceptron Learning Algorithm</title>
      <link>https://jmchung.github.io/post/perceptron-learning-algorithm/</link>
      <pubDate>Tue, 07 May 2019 00:51:00 +0800</pubDate>
      
      <guid>https://jmchung.github.io/post/perceptron-learning-algorithm/</guid>
      <description>This note illustrates the use of perceptron learning algorithm to identify the discriminant function with weight to partition the linearly separable data step-by-step. The material mainly outlined in Kröse et al. [1] work, and the example is from the Janecek&amp;rsquo;s [2] slides.
In machine learning, the perceptron is an supervised learning algorithm used as a binary classifier, which is used to identify whether a input data belongs to a specific group (class) or not.</description>
    </item>
    
    <item>
      <title>Using Amazon S3 as a Private Maven Repository</title>
      <link>https://jmchung.github.io/post/using-amazon-s3-as-a-private-maven-repository/</link>
      <pubDate>Sun, 01 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://jmchung.github.io/post/using-amazon-s3-as-a-private-maven-repository/</guid>
      <description>This article is a quick tutorial to setup a private maven repository using Amazon S3 instead of Nexus or Artifactory.
First we need to create a S3 bucket used to store two types of maven artifacts: stores two types of artifacts: releases and snapshots.
repository.example.com/[snapshots, releases]  Since we want a private repository, we can securely control access to this bucket for our users by leveraging AWS Identity and Access Management (IAM).</description>
    </item>
    
    <item>
      <title>Apache Shiro NullPointerException after Logout on GlassFish 4.1</title>
      <link>https://jmchung.github.io/post/apache-shiro-nullpointerexception-after-logout-on-glassfish-4.1/</link>
      <pubDate>Tue, 28 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jmchung.github.io/post/apache-shiro-nullpointerexception-after-logout-on-glassfish-4.1/</guid>
      <description>I&amp;rsquo;m currently working on a web application on Java EE7 stack and I&amp;rsquo;ve integrated Apache Shiro with CAS for security. Everything goes well, except the GlassFish 4.1 (build 13) server logs keep getting filled with following errors when calling logout() method:
Info: Session event listener threw exception java.lang.NullPointerException at org.jboss.weld.servlet.WeldTerminalListener.getSessionContext(WeldTerminalListener.java:65) at org.jboss.weld.servlet.WeldTerminalListener.sessionDestroyed(WeldTerminalListener.java:57) at org.apache.catalina.session.StandardSession.expire(StandardSession.java:910) at org.apache.catalina.session.StandardSession.expire(StandardSession.java:854) at org.apache.catalina.session.StandardSession.expire(StandardSession.java:842) at org.apache.catalina.session.StandardSession.invalidate(StandardSession.java:1603) at org.apache.catalina.session.StandardSessionFacade.invalidate(StandardSessionFacade.java:204) at org.apache.shiro.web.session.HttpServletSession.stop(HttpServletSession.java:113) at org.apache.shiro.session.ProxiedSession.stop(ProxiedSession.java:107) at org.apache.shiro.subject.support.DelegatingSubject$StoppingAwareProxiedSession.stop(DelegatingSubject.java:419) at org.</description>
    </item>
    
    <item>
      <title>Integrating Apache Shiro with CAS Authentication via LDAP</title>
      <link>https://jmchung.github.io/post/integrating-apache-shiro-with-cas-authentication-via-ldap/</link>
      <pubDate>Fri, 03 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jmchung.github.io/post/integrating-apache-shiro-with-cas-authentication-via-ldap/</guid>
      <description>In this post I want to share how to setup SSO with CAS and LDAP authentication, and then demonstrate how to integrate Apache Shiro with CAS in the web application.
An Example of An Individual LDAP Entry The following entry which represented in LDIF would be used to login the CAS. In this case, the username will be the full email address (case-insensitive) and the password is the value of userPassword attribute.</description>
    </item>
    
    <item>
      <title>How to Customise the Jackson JSON ObjectMapper in Java EE Enterprise Application</title>
      <link>https://jmchung.github.io/post/how-to-customise-the-jackson-json-objectmapper-in-java-ee-enterprise-application/</link>
      <pubDate>Wed, 18 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>https://jmchung.github.io/post/how-to-customise-the-jackson-json-objectmapper-in-java-ee-enterprise-application/</guid>
      <description>Assume we have a naive User POJO class with a BSON Type ObjectId field, i.e., id.
public class User { private ObjectId id; private String username; private String password; private Date createdAt; public getters/setters; ...  We can expect the following outputs from our REST services which is not a String with 24 hex characters.
{ &amp;quot;_id&amp;quot;: { &amp;quot;new&amp;quot;: false, &amp;quot;machine&amp;quot;: 805608948, &amp;quot;timeSecond&amp;quot;: 1403022678, &amp;quot;inc&amp;quot;: -871980150, &amp;quot;time&amp;quot;: 1403022678000 }, &amp;quot;username&amp;quot;: &amp;quot;John Smith&amp;quot;, &amp;quot;password&amp;quot;: &amp;quot;am9obiBzbWl0aCBwYXNzd29yZA&amp;quot;, &amp;quot;createdAt&amp;quot;: 1403022678341 }  As we know, it can be solved by using the Jackson annotation for configuring Serializer class to serialize the associated value.</description>
    </item>
    
    <item>
      <title>Map-Reduce with MongoDB and Morphia</title>
      <link>https://jmchung.github.io/post/map-reduce-with-mongodb-and-morphia/</link>
      <pubDate>Mon, 30 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jmchung.github.io/post/map-reduce-with-mongodb-and-morphia/</guid>
      <description>Just a note to record the usage of Map-Reduce with MongoDB and Morphia. Firstly, add the Morphia dependency.
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.mongodb&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;mongo-java-driver&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.11.2&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.mongodb.morphia&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;morphia&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;0.105&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt;  The syntax of MapReduceCommand in Morphia as shown in MapReduceCommand.java:
public MapReduceCommand( DBCollection inputCollection, String map, String reduce, String outputCollection, OutputType type, DBObject query) { // Compiled Code }  Here we want to group the sales amount (i.e., subtotal) of each county and exclude the null-subtotal from collection.</description>
    </item>
    
    <item>
      <title>Using the MongoDB Aggregation Framework via MongoDB Asynchronous Java Driver</title>
      <link>https://jmchung.github.io/post/using-the-mongodb-aggregation-framework-via-mongodb-asynchronous-java-driver/</link>
      <pubDate>Sat, 28 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jmchung.github.io/post/using-the-mongodb-aggregation-framework-via-mongodb-asynchronous-java-driver/</guid>
      <description>Introduction I often use Morphia in projects for mapping Java objects to/from MongoDB and it&amp;rsquo;s Query API instead of building the complex DBObject query. However, the current version (v. 0.105) without aggregate command support. Fortunately, the MongoDB Asynchronous Java Driver provides the aggregate builder to construct complex pipelines of operators in fluent way. As usual, the following paragraphs will express some basic usages of mongodb-async-driver in aggregation pipeline framework through an example.</description>
    </item>
    
    <item>
      <title>Integrating Swagger into JAX-RS with Java EE 6 specification</title>
      <link>https://jmchung.github.io/post/integrating-swagger-into-jax-rs-with-java-ee-6-specification/</link>
      <pubDate>Sat, 14 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jmchung.github.io/post/integrating-swagger-into-jax-rs-with-java-ee-6-specification/</guid>
      <description>Introduction Swagger is an awesome framework we often used to describe, consume and visualize our RESTful web services. Typically, we use Tomcat with Jersey as a servlet, then specify the Swagger package and Swagger Configuration class into web.xml, finally annotate the resources, methods and models to complete the configurations. Our team recently built a Java EE 7 application for a RESTful web service. The goal of this article is to share our experiences of configuring Swagger in Glassfish 4 without a web.</description>
    </item>
    
    <item>
      <title>How to solve jsoup does not get complete HTML document</title>
      <link>https://jmchung.github.io/post/how-to-solve-jsoup-does-not-get-complete-html-document/</link>
      <pubDate>Fri, 25 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jmchung.github.io/post/how-to-solve-jsoup-does-not-get-complete-html-document/</guid>
      <description>jsoup: Java HTML Parser.
 Where crawling web pages by using jsoup, it only returns parts of HTML content if the document size is too large, e.g., the below example transferred over 6MB content. According to the jsoup&amp;rsquo;s API Reference the default maximum is 1MB. So that we can set jsoup connection with maxBodySize to zero to get rid of this limitation and may accompany with sufficient timeout property.</description>
    </item>
    
    <item>
      <title>CentOS: Installing Apache Portable Runtime (APR) for Tomcat</title>
      <link>https://jmchung.github.io/post/centos-installing-apache-portable-runtime-apr-for-tomcat/</link>
      <pubDate>Fri, 06 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jmchung.github.io/post/centos-installing-apache-portable-runtime-apr-for-tomcat/</guid>
      <description>Introduction In Tomcat, the default HTTP Connector is BIO (Blocking I/O) connector with stability, low concurrency characteristics. To boost the Tomcat performance, the alternative ways either adapt NIO (Non-Blocking I/O) or APR (Apache Portable Runtime) connector. Especially, the APR performance is generally better than others when using SSL protocol. For more details on performance among these connectors can reference the Mike Noordermeer&amp;rsquo;s comparison.
Prerequisites for installing APR  APR library APR-util library OpenSSL library  To begin our installation, we&amp;rsquo;ll first need to install the OpenSSL to the server because we install CentOS 6.</description>
    </item>
    
    <item>
      <title>Automatically Mount an EBS Volume Upon Starting an Amazon EC2 Linux Instance</title>
      <link>https://jmchung.github.io/post/automatically-mount-an-ebs-volume-upon-starting-an-amazon-ec2-linux-instance/</link>
      <pubDate>Tue, 27 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jmchung.github.io/post/automatically-mount-an-ebs-volume-upon-starting-an-amazon-ec2-linux-instance/</guid>
      <description>Brief note to create an EBS volume then mount it while CentOS 6.3 on EC2 at boot.
 Show the space available on that filesystem before we mount the volume:
# df -h Filesystem Size Used Avail Use% Mounted on /dev/xvde1 7.9G 4.0G 3.5G 54% / tmpfs 296M 0 296M 0% /dev/shm  After we attach the EBS volume to an EC2 instance:
# fdisk -l Disk /dev/xvde: 9663 MB, 9663676416 bytes 255 heads, 63 sectors/track, 1174 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x00076be1 Device Boot Start End Blocks Id System /dev/xvde1 * 1 1045 8387584 83 Linux /dev/xvde2 1045 1175 1048576 82 Linux swap / Solaris Disk /dev/xvdt: 107.</description>
    </item>
    
    <item>
      <title>Using MongoDB with Morphia</title>
      <link>https://jmchung.github.io/post/using-mongodb-with-morphia/</link>
      <pubDate>Thu, 15 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jmchung.github.io/post/using-mongodb-with-morphia/</guid>
      <description>Morphia - The JVM Object Document Mapper for MongoDB.
 MongoDB is an extremely useful document-based database in NoSQL field. There are various drivers and client libraries in MongoDB Ecosystem and detailed manual to facilitate the developers to get into it shortly. In the beginning, we use MongoDB Java driver to manipulate CRUD operations on databases. Everything is running well but lots of BasicDBObject cause the code lack of readability and fluency.</description>
    </item>
    
    <item>
      <title>Converting ISODate from MongoDB</title>
      <link>https://jmchung.github.io/post/converting-isodate-from-mongodb/</link>
      <pubDate>Wed, 06 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jmchung.github.io/post/converting-isodate-from-mongodb/</guid>
      <description>I’m confused about the insertion operation with Date object via mongo-java-driver that always short of 8 hours where my place of residence is Taiwan (GMT+8). According to the enclosed references, we can observe that the incoming Date object will be set to ISO_8601_DATE_FORMAT as shown in the following code snippet.
if (o instanceof Date) { Date d = (Date) o; SimpleDateFormat format = new SimpleDateFormat(ISO_8601_DATE_FORMAT); serialize(new BasicDBObject(&amp;quot;$date&amp;quot;, format.format(d)), buf); return; }  An example next demonstrates that the given Date will plus 8 hours in the GMT+8 time zone.</description>
    </item>
    
    <item>
      <title>Jersey Test Framework with Maven</title>
      <link>https://jmchung.github.io/post/jersey-test-framework-with-maven/</link>
      <pubDate>Tue, 05 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>https://jmchung.github.io/post/jersey-test-framework-with-maven/</guid>
      <description>Introduction This memo records the issues while executing the unit-test with Jersey Test framework. We use the Jersey framework to implement the RESTful Web services and employ the Maven to manage the dependencies in project. First of all, we add the jersey-test-framework-grizzly2 dependency to enable the test framework in pom.xml; Second, we deploy the application using Jersey specific servlet in web.xml. Finally, we have the following java files within Maven Archetype – maven-archetype-webapp:</description>
    </item>
    
    <item>
      <title>A Jersey POJOMapping Example in Mapping Form Parameters</title>
      <link>https://jmchung.github.io/post/a-jersey-pojomapping-example-in-mapping-form-parameters/</link>
      <pubDate>Mon, 29 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jmchung.github.io/post/a-jersey-pojomapping-example-in-mapping-form-parameters/</guid>
      <description>Jersey, RESTful Web Services in Java.
 In Java Servlet circumstance, we usually harvest the form parameters by using request.getParameter(“FORM_FIELD_NAME”) syntax. Now we can do it more elegant while enabling Jsersey’s POJOMapping features. The following example demonstrates the account registration scenario. Here we have a Account class, i.e., Account.java:
public class Account { private String email; @JsonProperty(&amp;quot;email&amp;quot;) public String getEmail() { return email; } public void setEmail(String email) { this.</description>
    </item>
    
    <item>
      <title>Running Hadoop on CentOS 6 (Multi-Node Cluster)</title>
      <link>https://jmchung.github.io/post/running-hadoop-on-centos-6-multi-node-cluster/</link>
      <pubDate>Mon, 08 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jmchung.github.io/post/running-hadoop-on-centos-6-multi-node-cluster/</guid>
      <description>This demonstration has been tested with the following software versions and network settings:
 Oracle VM VirtualBox 4.1.22 r80657 CentOS-6.3_x86_64 with Minimal Installation Hadoop 1.0.3     Hostname IP Address Roles     hdp01 192.168.1.39 NameNode, JobTracker and DataNode   hdp02 192.168.1.40 DataNode, TaskTracker   hdp03 192.168.1.41 DataNode, TaskTracker    Candidate CentOS VMs After installing the CentOS in VirtualBox, patch up system by applying all updates and install wget tool for future use.</description>
    </item>
    
  </channel>
</rss>